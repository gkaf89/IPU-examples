resnet8_test:
  model: resnet
  model_size: 8
  dataset: cifar-10
  ckpts_per_epoch: 1
  logs_per_epoch: 1
  enable_half_partials: True
  gradient_accumulation_count: 1
  replicas: 1
  epochs: 10
  validation: False

resnet50_mlperf_pod16: &resnet50_mlperf_pod16
  model: resnet
  model_size: 50
  replicas: 16
  micro_batch_size: 16
  pipeline: True
  shards: 1
  pipeline_splits: [
    stage1/unit1/relu,
    stage1/unit3/relu,
    stage2/unit4/relu]
  gradient_accumulation_count: 9
  enable_recomputation: True
  optimiser: momentum
  momentum: 0.9
  normalise_input: True
  pipeline_schedule: Sequential
  available_memory_proportion: [0.2]
  epochs_per_ckpt: 4
  batch_norm: True
  dataset: imagenet
  data_dir: /localdata/datasets/
  internal_exchange_optimisation_target: memory
  enable_half_partials: True
  eight_bit_io: True
  lr_schedule: polynomial_decay_lr
  warmup_epochs: 5
  abs_learning_rate: 4.588
  abs_end_learning_rate: 0.0001
  epochs: 44
  label_smoothing: 0.1
  weight_decay: 2.5e-05
  BN_decay: 0.9
  BN_span: 2
  disable_variable_offloading: True
  mlperf_logging: True
  standard_imagenet : True
  max_cross_replica_buffer_size: 52428800
  gather_conv_output: False
  logs_per_epoch: 5
  fused_preprocessing: True

resnet50_mlperf_pod16_bs20:
  <<: *resnet50_mlperf_pod16
  gradient_accumulation_count: 6
  micro_batch_size: 20
  available_memory_proportion: [0.15]
  gather_conv_output: True
  abs_learning_rate: 3.824
  epochs: 40
  pipeline_splits: [
    stage1/unit1/relu,
    stage1/unit3/relu,
    stage2/unit1/relu,
    stage2/unit3/relu,
    stage3/unit1/relu,
    stage3/unit4/relu]

resnet50_mlperf_pod64_bs20:
  <<: *resnet50_mlperf_pod16
  gradient_accumulation_count: 2
  micro_batch_size: 20
  available_memory_proportion: [0.145]
  disable_variable_offloading: True
  gather_conv_output: True
  abs_learning_rate: 5.098
  epochs: 41
  ckpt_epochs_offset: 1
  replicas: 64
  device_iterations: 100000
  pipeline_splits: [
    stage1/unit1/relu,
    stage1/unit3/relu,
    stage2/unit1/relu,
    stage2/unit3/relu,
    stage3/unit1/relu,
    stage3/unit4/relu]

resnet50_base: &resnet50_base
  model: resnet
  model_size: 50
  dataset: imagenet
  data_dir: /localdata/datasets/
  stable_norm: True
  internal_exchange_optimisation_target: balanced
  enable_half_partials: True
  normalise_input: True
  enable_recomputation: True
  ckpts_per_epoch: 1
  logs_per_epoch: 1
  eight_bit_io: True
  weight_decay: 0.0001
  loss_scaling: 128
  epochs: 100

resnet50_gn_16ipus: &resnet50_gn_16ipus
  <<: *resnet50_base
  micro_batch_size: 8
  gradient_accumulation_count: 8
  replicas: 16
  optimiser: momentum
  momentum: 0.9
  epochs: 65
  lr_schedule: cosine
  label_smoothing: 0.1

resnet50_bn_16ipus: &resnet50_bn_16ipus
  <<: *resnet50_base
  micro_batch_size: 16
  gradient_accumulation_count: 256
  replicas: 4
  optimiser: momentum
  momentum: 0.9
  pipeline_num_parallel: 32
  pipeline_schedule: Grouped
  shards: 4
  pipeline_splits: [
    stage1/unit3/relu,
    stage2/unit4/relu,
    stage3/unit4/relu]
  pipeline: True
  batch_norm: True
  available_memory_proportion: ["0.15"]
  disable_variable_offloading: True
  precision: "16.32"
  lr_schedule: cosine
  base_learning_rate_exponent: -13
  label_smoothing: 0.1

resnet50_bn_32ipus:
  <<: *resnet50_bn_16ipus
  replicas: 8
  gradient_accumulation_count: 128

resnet50_bn_64ipus:
  <<: *resnet50_bn_16ipus
  replicas: 16
  gradient_accumulation_count: 64

resnext101_16ipus:
  model: resnext
  model_size: 101
  dataset: imagenet
  data_dir: /localdata/datasets
  shards: 2
  replicas: 8
  micro_batch_size: 6
  gradient_accumulation_count: 16
  epochs: 120
  enable_recomputation: True
  pipeline_splits: [stage3/unit4/relu]
  pipeline_schedule: Grouped
  pipeline: True
  optimiser: momentum
  momentum: 0.9
  ckpts_per_epoch: 1
  internal_exchange_optimisation_target: balanced
  disable_variable_offloading: True
  normalise_input: True
  stable_norm: True
  base_learning_rate_exponent: -11

  enable_half_partials: True
  lr_schedule: cosine
  label_smoothing: 0.1
  eight_bit_io: True

efficientnet_b4_16ipus_base: &efficientnet_b4_16ipus_base
  model: efficientnet
  model_size: 4
  dataset: imagenet
  data_dir: /localdata/datasets
  precision: "16.32"
  groups: 4
  optimiser: RMSprop
  lr_schedule: exponential
  enable_recomputation: True
  enable_conv_dithering: True
  available_memory_proportion: ["0.15"]
  internal_exchange_optimisation_target: balanced
  pipeline_schedule: Grouped
  weight_avg_exp: [0.97]
  enable_half_partials: True
  cutmix_lambda: 0.85
  cutmix_version: 1
  mixup_alpha: 0.2
  disable_variable_offloading: True
  shards: 4
  pipeline: True
  replicas: 4
  eight_bit_io: True

efficientnet_b4_g1_16ipus: &efficientnet_b4_g1_16ipus
  <<: *efficientnet_b4_16ipus_base
  group_dim: 1
  expand_ratio: 6
  micro_batch_size: 3
  gradient_accumulation_count: 64
  pipeline_splits: [block2b, block4b, block6c]

efficientnet_b4_g1_32ipus:
  <<: *efficientnet_b4_g1_16ipus
  gradient_accumulation_count: 32
  replicas: 8

efficientnet_b4_g1_64ipus:
  <<: *efficientnet_b4_g1_16ipus
  gradient_accumulation_count: 16
  replicas: 16

efficientnet_b4_g16_large_batch_size_high_throughput: &efficientnet_b4_g16_large_batch_size_high_throughput
  <<: *efficientnet_b4_16ipus_base
  precision: "16.16"
  standard_imagenet: True
  optimiser: LARS
  lr_schedule: polynomial_decay_lr
  internal_exchange_optimisation_target: memory
  available_memory_proportion: ["0.4"]
  pipeline_splits: [block4a]
  shards: 2
  group_dim: 16
  expand_ratio: 4
  label_smoothing: 0.1
  warmup_epochs: 70
  micro_batch_size: 3
  base_learning_rate_exponent: -9.5
  momentum: 0.885
  lars_weight_decay: 2.5e-6

efficientnet_b4_g16_16ipus:
  <<: *efficientnet_b4_g16_large_batch_size_high_throughput
  replicas: 8
  gradient_accumulation_count: 256

efficientnet_b4_g16_32ipus:
  <<: *efficientnet_b4_g16_large_batch_size_high_throughput
  replicas: 16
  gradient_accumulation_count: 128

efficientnet_b4_g16_64ipus:
  <<: *efficientnet_b4_g16_large_batch_size_high_throughput
  replicas: 32
  gradient_accumulation_count: 64

resnet50_mlperf_pod16_lars: &resnet50_mlperf_pod16_lars
  <<: *resnet50_mlperf_pod16
  model_size: 50
  replicas: 16
  pipeline: True
  shards: 1
  pipeline_splits: [
    stage1/unit1/relu,
    stage1/unit3/relu,
    stage2/unit1/relu,
    stage2/unit3/relu,
    stage3/unit1/relu,
    stage3/unit4/relu]
  enable_recomputation: True
  optimiser: LARS
  momentum: 0.9
  normalise_input: True
  pipeline_schedule: Sequential
  available_memory_proportion: [0.15]
  batch_norm: True
  dataset: imagenet
  internal_exchange_optimisation_target: memory
  enable_half_partials: True
  eight_bit_io: True
  lr_schedule: polynomial_decay_lr
  mlperf_logging: True
  standard_imagenet: True
  gather_conv_output: True
  abs_end_learning_rate: 0.0001
  warmup_epochs: 2
  label_smoothing: 0.1
  weight_decay: 0.00005
  BN_decay: 0.9
  BN_span: 2
  fused_preprocessing: True
  loss_scaling: 128
  abs_learning_rate: 10.72
  gradient_accumulation_count: 10
  epochs: 38
  micro_batch_size: 22
  disable_variable_offloading: True
  lars_weight_decay: 0.00005
  max_reduce_many_buffer_size: 216

resnet50_mlperf_pod32_lars:
  <<: *resnet50_mlperf_pod16_lars
  gradient_accumulation_count: 6
  replicas: 32
  epochs: 38
  abs_learning_rate: 11.42
  max_reduce_many_buffer_size: 216

resnet50_mlperf_pod64_lars:
  <<: *resnet50_mlperf_pod16_lars
  micro_batch_size: 20
  gradient_accumulation_count: 4
  replicas: 64
  epochs: 40
  abs_learning_rate: 14.225
  max_reduce_many_buffer_size: 216
  warmup_epochs: 2

efficientnet_b4_g1_with_batch_norm_base: &efficientnet_b4_g1_with_batch_norm_base
# modification for EfficientNet-B4 with batch norm this config entry
# should be used in combination with another EfficientNet B4 config to be
# valid. (e.g. efficientnet_b4_16ipus_base, efficientnet_b4_g16_large_batch_size_high_throughput)
  data_dir: ./datasets
  executable_cache_path: ./poplar_executable_cache
  gather_conv_output: True
  fused_preprocessing: True
  normalise_input: True
  eight_bit_io: True
  group_dim: 1
  shards: 1
  expand_ratio: 6
  batch_norm: True
  force_weight_to_fp32:
    - batch_norm/moving_variance
  only_use_slic_vmac_16: True
  max_cross_replica_buffer_size: 38934496
  stable_norm: False
  force_unstable_norm: True
  gcl_max_broadcast_size: 100000
  min_remote_tensor_size: 65536
  max_reduce_many_buffer_size: 1000000000
  pipeline_schedule: Sequential
  pipeline_splits: [block0, block2a, block2a/b, block2b, block2c, block2d, block3a, block3c, block4a, block4c, block5a, block5d, block6a, block6e]

efficientnet_b4_g1_with_batch_norm_16ipus: &efficientnet_b4_g1_with_batch_norm_16ipus
# 16.16 POD16 based on the G16 config uses the LARS optimiser
  <<: *efficientnet_b4_g16_large_batch_size_high_throughput
  <<: *efficientnet_b4_g1_with_batch_norm_base
  available_memory_proportion: ["0.15"]
  disable_variable_offloading: False
  BN_span: 8
  micro_batch_size: 6
  replicas: 16
  gradient_accumulation_count: 64
  base_learning_rate_exponent: -10
  weight_decay: 0
  name_suffix: _LARS

efficientnet_b4_g1_with_batch_norm_pod64_modification: &efficientnet_b4_g1_with_batch_norm_pod64_modification
  replicas: 64
  identical_replica_seeding: True
  seed: 1
  gradient_mean_reduce_re:
    - ".*"

efficientnet_b4_g1_with_batch_norm_64ipus: &efficientnet_b4_g1_with_batch_norm_64ipus
  <<: *efficientnet_b4_g1_with_batch_norm_16ipus
  <<: *efficientnet_b4_g1_with_batch_norm_pod64_modification
  gradient_accumulation_count: 16

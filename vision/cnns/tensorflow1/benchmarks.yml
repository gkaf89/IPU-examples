---
common_options: &common_options
   location: training
   output:
      - [samples/sec, 'throughput']
      - [accuracy, 'accuracy']
      - [loss, 'loss']

tf1_resnet50_train_synth_1ipu:
   location: training
   description:
      ResNet training on a single Mk2 IPU, synthetic.
   cmd: >-
      python
         train.py
         --config resnet50_mlperf_pod16
         --replicas 1
         --epochs 1
         --no-validation
         --gradient-accumulation-count 144
         --synthetic-data
         --logs-per-epoch 16
         --BN-span 1
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 1
   output:
      - [samples/sec, 'throughput']

tf1_resnet50_train_real_pod4:
   <<: *common_options
   description: |
      ResNet50 training with real data on 4 IPUs.
   cmd: >-
      poprun
         -vv
         --mpi-local-args="-x TF_POPLAR_FLAGS -x POPLAR_RUNTIME_OPTIONS"
         --num-instances 2
         --num-replicas 4
      python3
         train.py
         --config resnet50_mlperf_pod16_lars
         --identical-replica-seeding
         --data-dir $DATASETS_DIR/
         --no-validation
         --epochs 1
         --epochs-per-ckpt 0
         --gradient-accumulation-count 40
   env:
      TF_POPLAR_FLAGS: "--executable_cache_path=/tmp/tf_cache/"
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 4
      accuracy:
         reduction_type: 'final'
         regexp: 'accuracy: *(.*?) \%'
         skip: 2
      loss:
         reduction_type: 'final'
         regexp: 'loss: *(\d*\.\d*)'
         skip: 2

tf1_resnet50_train_real_pod16_conv:
   <<: *common_options
   description: |
      ResNet50 training with real data on 16 IPUs.
   cmd: >-
      poprun
         -vv
         --mpi-local-args="
           -x TF_POPLAR_FLAGS
           -x POPLAR_RUNTIME_OPTIONS
           -x POPLAR_ENGINE_OPTIONS"
         --num-instances 8
         --num-replicas 16
      python3
         train.py
         --config resnet50_mlperf_pod16_lars
         --identical-replica-seeding
         --seed 1
         --data-dir $DATASETS_DIR/
         --wandb
   env:
      TF_POPLAR_FLAGS: "--executable_cache_path=./tf_cache/"
      POPLAR_ENGINE_OPTIONS: '{"target.hostSyncTimeout":"3000"}'
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 16
      accuracy:
         reduction_type: 'final'
         regexp: 'accuracy: *(.*?) \%'
         skip: 8
      loss:
         reduction_type: 'final'
         regexp: 'loss: *(\d*\.\d*)'
         skip: 8

tf1_resnet50_train_real_pod64_conv:
   <<: *common_options
   description: |
      ResNet50 training with real data, testing convergence.
      Need to have a comma separated HOSTS env var set to
      run this benchmark as it requires multiple hosts.
   cmd: >-
      poprun
         -vv
         --host $HOSTS
         --num-instances 32
         --num-replicas 64
         --ipus-per-replica 1
         --num-ilds 1
         --vipu-server-host "$VIPU_CLI_API_HOST"
         --vipu-partition=$PARTITION
         --vipu-cluster=$CLUSTER
         --update-partition=yes
         --remove-partition=yes
         --reset-partition=no
         --sync-type=ST_POD_NATIVE_DEFAULT
         --vipu-server-timeout 300
         --mpi-global-args="
            --mca oob_tcp_if_include $TCP_IF_INCLUDE
            --mca btl_tcp_if_include $TCP_IF_INCLUDE"
         --mpi-local-args="
            -x OPAL_PREFIX
            -x LD_LIBRARY_PATH
            -x PATH
            -x PYTHONPATH
            -x IPUOF_VIPU_API_TIMEOUT=600
            -x POPLAR_LOG_LEVEL=WARN
            -x TF_POPLAR_FLAGS
            -x DATASETS_DIR
            -x POPLAR_ENGINE_OPTIONS
            -x POPLAR_RUNTIME_OPTIONS"
      python3
         train.py
         --config resnet50_mlperf_pod64_lars
         --identical-replica-seeding
         --seed 1
         --data-dir $DATASETS_DIR/
         --logs-path .
         --wandb
   env:
      TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
      POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false",
         "target.hostSyncTimeout":"3000"}'
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 64
      accuracy:
         reduction_type: 'final'
         regexp: 'accuracy: *(.*?) \%'
         skip: 64
      loss:
         reduction_type: 'final'
         regexp: 'loss: *(\d*\.\d*)'
         skip: 64

tf1_resnet50_train_real_pod256_conv:
  <<: *common_options
  description: |
    ResNet50 validation with real data testing convergence on 256 IPUs.
  cmd: >-
    poprun
      -vv
      --host $HOSTS
      --ipus-per-replica 1
      --num-ilds 4
      --num-instances 128
      --num-replicas 256
      --update-partition=yes
      --reset-partition=no
      --vipu-server-timeout 600
      --vipu-server-host "$VIPU_SERVER_HOST"
      --vipu-partition=$PARTITION
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x TF_POPLAR_FLAGS
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_RUNTIME_OPTIONS"
    python3 train.py
      --config resnet50_mlperf_pod64_lars
      --identical-replica-seeding
      --data-dir $DATASETS_DIR/
      --logs-path .
      --identical-replica-seeding
      --seed 1
      --wandb
      --gradient-accumulation-count 2
      --replicas 256
      --epochs 45
      --abs-learning-rate 20.2
      --max-reduce-many-buffer-size 216
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false",
        "target.hostSyncTimeout":"3000"}'
  data:
    throughput:
        regexp: 'throughput: *(.*?) samples\/sec,'
        skip: 64
    accuracy:
        reduction_type: 'final'
        regexp: 'accuracy: *(.*?) \%'
    loss:
        reduction_type: 'final'
        regexp: 'loss: *(\d*\.\d*)'

tf1_resnext101_train_real_pod4:
   <<: *common_options
   description:
      ResNeXt101 training pipelined over 2*2 IPUs.
   cmd: >-
      poprun
         -vv
         --mpi-local-args="-x POPLAR_RUNTIME_OPTIONS"
         --num-replicas 2
         --ipus-per-replica 2
         --num-instances 2
      python3 train.py
         --config resnext101_16ipus
         --replicas 2
         --ckpts-per-epoch 0
         --epochs 1
         --logs-per-epoch 16
         --no-validation
         --data-dir $DATASETS_DIR
         --gradient-accumulation-count 64
         --num-io-tiles 32
         --prefetch-depth 4
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 17
      accuracy:
         reduction_type: 'final'
         regexp: 'accuracy: *(.*?) \%'
         skip: 17
      loss:
         reduction_type: 'final'
         regexp: 'loss: *(\d*\.\d*)'
         skip: 17

tf1_resnet50_serving_infer_real_1ipu:
  description: |
    Resnet50 batch-sizes 1, 8, 16 serving on 1 IPU
  parameters:
    batchsize: 1,8,16
  cmd: >-
    python3 send_request.py resnet50 images
      --port=8504
      --num-threads 16
      --num-images 1600
      --batch-size {batchsize}
      --model-batch-size 16
  location: inference
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
  output:
    - [samples/sec, 'throughput']

tf1_resnext101_train_real_pod16:
   <<: *common_options
   description:
      ResNeXt101 training pipelined over 2*8 IPUs.
   cmd: >-
      poprun
        -vv
        --mpi-local-args="
          -x TF_POPLAR_FLAGS
          -x POPLAR_RUNTIME_OPTIONS"
        --num-replicas 8
        --ipus-per-replica 2
        --num-instances 8
      python3 train.py
         --config resnext101_16ipus
         --ckpts-per-epoch 0
         --epochs 1
         --logs-per-epoch 16
         --no-validation
         --data-dir $DATASETS_DIR
         --num-io-tiles 32
         --prefetch-depth 4
   env:
      TF_POPLAR_FLAGS: "--executable_cache_path=/tmp/tf_cache/"
   data:
      throughput:
         regexp: 'throughput: *(.*?) samples\/sec,'
         skip: 8
      accuracy:
         reduction_type: 'final'
         regexp: 'accuracy: *(.*?) \%'
         skip: 8
      loss:
         reduction_type: 'final'
         regexp: 'loss: *(\d*\.\d*)'
         skip: 8

tf1_efficientnet_b4_train_real_pod16:
  description:
    EfficientNet-B4 training pipelined on 16x IPUs.
  cmd: >-
    poprun
      -vv
      --mpi-local-args="-x POPLAR_RUNTIME_OPTIONS"
      --num-replicas 4
      --num-instances 4
      --ipus-per-replica 4
    python3 train.py
      --config efficientnet_b4_g1_16ipus
      --identical-replica-seeding
      --epochs 1
      --data-dir $DATASETS_DIR/
      --logs-per-epoch 16
      --epochs-per-ckpt 1
      --no-validation
      --eight-bit-io
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
      skip: 10
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_train_real_pod16:
  description:
    GS-16 EfficientNet-B4 training pipelined on 16 IPUs.
  cmd: >-
    poprun
      -vv
      --mpi-local-args="-x POPLAR_RUNTIME_OPTIONS"
      --num-replicas 8
      --num-instances 8
      --ipus-per-replica 2
    python3 train.py
      --config efficientnet_b4_g16_16ipus
      --identical-replica-seeding
      --epochs 1
      --data-dir $DATASETS_DIR/
      --logs-per-epoch 16
      --epochs-per-ckpt 1
      --no-validation
      --dataset-percentage-to-use 15
      --eight-bit-io
      --num-io-tiles 32
      --prefetch-depth 3
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
      skip: 10
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_train_real_pod64_conv:
  description:
    GS-16 EfficientNet-B4 training, testing convergence on Pod64
  cmd: >-
    poprun
      --vv
      --host $HOSTS
      --num-instances 32
      --num-replicas 32
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 2
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x TF_POPLAR_FLAGS
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 train.py
      --config efficientnet_b4_g16_64ipus
      --identical-replica-seeding
      --epochs 350
      --logs-per-epoch 2
      --epochs-per-ckpt 1
      --no-validation
      --data-dir $DATASETS_DIR
      --wandb
      --eight-bit-io
      --seed 1
      --num-io-tiles 32
      --prefetch-depth 3
      --log-dir ./logs/checkpoints
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
      skip: 10
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_train_real_pod256_conv:
  description:
    GS-16 EfficientNet-B4 training, testing convergence on Pod256
  cmd: >-
    poprun
      --vv
      --host $HOSTS
      --num-instances 128
      --num-replicas 128
      --num=ilds 4
      --ipus-per-replica 2
      --reset-partition=no
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x TF_POPLAR_FLAGS
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 train.py
      --config efficientnet_b4_g16_64ipus
      --identical-replica-seeding
      --epochs 350
      --logs-per-epoch 2
      --epochs-per-ckpt 1
      --no-validation
      --data-dir $DATASETS_DIR
      --wandb
      --eight-bit-io
      --seed 1
      --num-io-tiles 32
      --prefetch-depth 3
      --log-dir ./logs/checkpoints
      --replicas 128
      --gradient-accumulation-count 16
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
  location:
    examples_internal/vision/cnns/tensorflow1/training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_validation_real_pod16_conv:
  description:
    GS-16 EfficientNet-B4 validation, testing convergence on Pod16
  cmd: >-
    poprun
      --vv
      --num-instances 1
      --num-replicas 8
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 1
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x TF_POPLAR_FLAGS
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 train.py
      --config efficientnet_b4_g16_256ipus
      --identical-replica-seeding
      --epochs 350
      --logs-per-epoch 2
      --epochs-per-ckpt 1
      --no-validation
      --data-dir $DATASETS_DIR
      --wandb
      --eight-bit-io
      --identical-replica-seeding
      --seed 1
      --num-io-tiles 32
      --prefetch-depth 3
      --log-dir ./logs
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss:\s*([\d\.]+|nan)\,'
      skip: 10
  output:
    - [samples/sec, 'throughput']

G16_EfficientNet_B4_pipelined_validation_convergence:
  description:
    GS-16 EfficientNet-B4 validation, testing convergence on Pod16
  cmd: >-
    python3
      validation.py
      --config efficientnet_b4_g16_16ipus
      --data-dir $DATASETS_DIR/imagenet-data
      --restore-path ./logs/checkpoints
      --log-dir ./logs/checkpoints
      --epochs-per-ckpt 1
      --logs-per-epoch 2
      --batch-size 25
      --shards 1
      --wandb
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/localdata/$USER/tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'top-1 accuracy: *(.*?) \%'
  output:
    - [accuracy, 'accuracy']

tf1_resnet50_infer_synth_1ipu:
  description: |
    Resnet50 batch-sizes 1 to 64 inference on 1 IPU
    using synthetic data created on the IPU
  parameters:
    batchsize: 1,4,16,32,64,90
  cmd: >-
    python3 validation.py
      --model resnet
      --model-size 50
      --dataset imagenet
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']

tf1_resnet50_infer_gen_pod4:
  description: |
    Resnet50 batch-sizes 1 to 80 inference on 1/4 IPUs with data
    generated on the host.
  parameters:
    batchsize: 1,4,16,32,64,80
    replicas: 1,4
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
    python3 validation.py
      --model resnet
      --model-size 50
      --dataset imagenet
      --micro-batch-size {batchsize}
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 64
      --prefetch-depth 2
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']

tf1_resnet50_low_latency_infer_gen_pod4:
  description: |
    Resnet50 batch-sizes 1 to 64 inference on 4 IPUs with data
    generated on the host. Using embedded runtime
  parameters:
    batchsize: 1,2,4,8,16,32,64
  cmd: >-
    poprun
      -vv
      --mpi-local-args="-x TF_POPLAR_FLAGS -x POPLAR_RUNTIME_OPTIONS"
      --num-instances 4
      --num-replicas 4
    python3 inference_embedded.py
      --model resnet
      --config resnet50_base
      --dataset imagenet
      --generated-data
      --iterations 8000
      --device-iterations 1000
      --micro-batch-size {batchsize}
      --eight-bit-io
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 32
      --prefetch-depth 2
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_resnext101_infer_gen_1ipu:
  description: |
    Resnext101 batch-sizes 1 and 16 inference on 1 IPU using synthetic data
    created on the IPU
  parameters:
    batchsize: 1,16
  cmd: >-
    python3 validation.py
      --model resnext
      --model-size 101
      --dataset imagenet
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_resnext101_long_infer_synth_1ipu:
  description: |
    Resnext101 batch-sizes 1 to 16 inference on 1 IPU using synthetic data
    created on the IPU
  parameters:
    batchsize: 1,2,4,8,16
  cmd: >-
    python3 validation.py
      --model resnext
      --model-size 101
      --dataset imagenet
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_resnext101_long_infer_gen_pod4:
  description: |
    Resnext101 batch-sizes 1 to 16 inference on 1/4 IPU using data generated
    on the host.
  parameters:
    batchsize: 1,2,4,8,16
    replicas: 1,4
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
    python3 validation.py
      --model resnext
      --model-size 101
      --dataset imagenet
      --micro-batch-size {batchsize}
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 64
      --prefetch-depth 4
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [samples/sec, 'throughput']

tf1_resnext101_low_latency_infer_gen_pod4:
  description: |
    Resnext101 batch-sizes 1 to 16 inference on 4 IPUs using data generated
    on the host.
  parameters:
    batchsize: 1,2,4,8,16
  cmd: >-
    poprun
      -vv
      --mpi-local-args="-x TF_POPLAR_FLAGS -x POPLAR_RUNTIME_OPTIONS"
      --num-instances 4
      --num-replicas 4
    python3 inference_embedded.py
      --model resnext
      --model-size 101
      --dataset imagenet
      --micro-batch-size {batchsize}
      --generated-data
      --batch-norm
      --eight-bit-io
      --enable-half-partials
      --fused-preprocessing
      --iterations 8000
      --device-iterations 1000
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 64
      --prefetch-depth 3
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b0_infer_synth_1ipu:
  description: |
    EfficientNet-B0 batch-sizes 1 and 36 inference on 1 IPU
    using synthetic data created on the IPU
  parameters:
    batchsize: 1,36
  cmd: >-
    python3 validation.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b0_long_infer_synth_1ipu:
  description: |
    EfficientNet-B0 batch-sizes 1 to 32 inference on 1 IPU using synthetic
    data created on the IPU
  parameters:
    batchsize: 1,8,16,32
  cmd: >-
    python3 validation.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b0_g16_infer_synth_1ipu:
  description: |
    GS-16 EfficientNet-B0 batch-sizes 1 to 88 inference on 1 IPU using
    synthetic data created on the IPU
  parameters:
    batchsize: 1,8,16,32,64,88
  cmd: >-
    python3 validation.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b0_long_infer_gen_pod4:
  description: |
    EfficientNet-B0 batch-sizes 1 to 32 inference on 1/4 IPUs using
    host-generated data.
  parameters:
    batchsize: 1,8,16,32
    replicas: 1,4
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
    python3 validation.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 64
      --prefetch-depth 4
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b0_infer_gen_pod4:
  description: |
    G16 EfficientNet-B0 batch-sizes 1 to 88 inference on 1/4 IPUs
    using host-generated data.
  parameters:
    batchsize: 1,8,16,32,64,88
    replicas: 1,4
  cmd:
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
    python3 validation.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 64
      --prefetch-depth 3
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b0_g16_low_latency_infer_gen_pod4:
  description: |
    EfficientNet-B0 batch-sizes 1 to 88 inference on 4 IPUs using
    host-generated data. Embedded runtime
  parameters:
    batchsize: 1,2,4,8,16,32,64,88
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np 4
      --bind-to socket
    python3 inference_embedded.py
      --model efficientnet
      --model-size 0
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --dataset imagenet
      --generated-data
      --batch-norm
      --iterations 8000
      --device-iterations 1000
      --micro-batch-size {batchsize}
      --eight-bit-io
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 64
      --prefetch-depth 2
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b0_low_latency_infer_gen_pod4:
  description: |
    EfficientNet-B0 batch-sizes 1 to 42 inference on 1/4 IPUs using
    host-generated data.
  parameters:
    batchsize: 1,2,4,8,16,32,40,42
  cmd: >-
    poprun
      -vv
      --mpi-local-args="
        -x TF_POPLAR_FLAGS
        -x POPLAR_RUNTIME_OPTIONS"
      --num-instances 4
      --num-replicas 4
    python3 inference_embedded.py
      --model efficientnet
      --model-size 0
      --dataset imagenet
      --generated-data
      --batch-norm
      --iterations 8000
      --device-iterations 1000
      --micro-batch-size {batchsize}
      --eight-bit-io
      --enable-half-partials
      --fused-preprocessing
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 32
      --prefetch-depth 2
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b4_infer_synth_1ipu:
  description: |
    EfficientNet-B4 batch-sizes 1 to 5 inference on 1 IPU using
    synthetic data created on the IPU
  parameters:
    batchsize: 1,2,4,5
  cmd: >-
    python3 validation.py
      --model efficientnet
      --model-size 4
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_infer_synth_1ipu:
  description: |
    G16 EfficientNet-B4 batch-sizes 1 to 20 inference on 1 IPU
    using synthetic data created on the IPU
  parameters:
    batchsize: 1,2,8,16,20
  cmd: >-
    python3 validation.py
      --model efficientnet
      --model-size 4
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --synthetic-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 1
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_infer_gen_pod4:
  description: |
    EfficientNet-B4 batch-sizes 1 to 5 inference on 1/4
    IPU using host generated data.
  parameters:
    batchsize: 1,2,4,5
    replicas: 1,4
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
      python3 validation.py
      --model efficientnet
      --model-size 4
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 32
      --prefetch-depth 3
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_g16_infer_synth_1ipu:
  description: |
    G16 EfficientNet-B4 batch-sizes 1 to 20 inference on 1/4
    IPU using host generated data.
  parameters:
    batchsize: 1,2,8,16,20
    replicas: 1,4
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np {replicas}
      --bind-to socket
      -x POPLAR_RUNTIME_OPTIONS
    python3 validation.py
      --model efficientnet
      --model-size 4
      --dataset imagenet
      --precision 16.16
      --micro-batch-size {batchsize}
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --generated-data
      --repeat 10
      --device-iterations 1000
      --batch-norm
      --enable-half-partials
      --eight-bit-io
      --fused-preprocessing
      --num-io-tiles 32
      --prefetch-depth 3
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
      skip: 4
  output:
    - [samples/sec, 'throughput']

tf1_efficientnet_b4_low_latency_infer_gen_pod4:
  description: |
    EfficientNet-B4 batch-sizes 1 to 5 inference on 4 IPUs using
    host-generated data.
  parameters:
    batchsize: 1,2,4,5
  cmd: >-
    mpirun
      --tag-output
      --allow-run-as-root
      --np 4
      --bind-to socket
    python3 inference_embedded.py
      --model efficientnet
      --model-size 4
      --dataset imagenet
      --generated-data
      --batch-norm
      --iterations 8000
      --device-iterations 1000
      --micro-batch-size {batchsize}
      --eight-bit-io
      --enable-half-partials
      --fused-preprocessing
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 32
      --prefetch-depth 3
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b4_g16_low_latency_infer_gen_pod4:
  description: |
    EfficientNet-B4 batch-sizes 1 to 20 inference on 4 IPUs using
    host-generated data.
  parameters:
    batchsize: 1,2,4,8,16,20
  cmd: >-
    poprun
      -vv
      --mpi-local-args="
        -x TF_POPLAR_FLAGS
        -x POPLAR_RUNTIME_OPTIONS"
      --num-instances 4
      --num-replicas 4
    python3 inference_embedded.py
      --model efficientnet
      --model-size 4
      --expand-ratio 4
      --depth-divisor 8
      --group-dim 16
      --dataset imagenet
      --generated-data
      --batch-norm
      --iterations 8000
      --device-iterations 1000
      --micro-batch-size {batchsize}
      --eight-bit-io
      --enable-half-partials
      --fused-preprocessing
      --no-dataset-cache
      --num-inference-thread 6
      --tmp-execs
      --num-io-tiles 64
      --prefetch-depth 2
  location: training
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"2"}'
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec,'
    latency:
      regexp: 'latency avg: *(.*?) ms,'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms,'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms,'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b7_infer_gen_pod4:
  description: |
    EfficientNet-B7 batch-sizes 1 and 2 inference on 4 IPU
    using host generated data
  parameters:
    batchsize: 1,2
  cmd: >-
    poprun
      -vv
      --mpi-local-args="
        -x TF_POPLAR_FLAGS
        -x POPLAR_RUNTIME_OPTIONS"
      --num-instances 4
      --num-replicas 4
    python3 inference_embedded.py
      --model efficientnet
      --model-size 7
      --dataset imagenet
      --generated-data
      --batch-norm
      --iterations 8000
      --device-iterations 100
      --micro-batch-size {batchsize}
      --eight-bit-io
      --enable-half-partials
      --fused-preprocessing
      --no-dataset-cache
      --num-inference-thread 3
      --tmp-execs
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
    latency:
      regexp: 'latency avg: *(.*?) ms'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms'
    latency_max:
      regexp: 'latency max: *(.*?) ms'
  output:
    - [Batch size, 'batchsize']
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']
    - [latency max(ms), 'latency_max']

tf1_efficientnet_b4_batchnorm_train_real_pod16:
  description:
    Standard EfficientNet-B4 with batch norm training replicated on 16 IPUs.
  cmd: >-
    poprun
      --num-instances=2
      --num-replicas=16
    python3 train.py
      --config efficientnet_b4_g1_with_batch_norm_16ipus
      --epochs 1
      --data-dir $DATASETS_DIR/
      --logs-per-epoch 16
      --seed=1
      --no-validation
  env:
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"unlimited"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 6
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
      skip: 10
  output:
    - [samples/sec, 'throughput']


tf1_efficientnet_b4_batchnorm_train_real_pod64_conv:
  description:
    Standard EfficientNet-B4 with batch norm, testing convergence on Pod64
  cmd: >-
    poprun
      --vv
      --host $HOSTS
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x TF_POPLAR_FLAGS
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
      --update-partition=no
      --reset-partition=no
      --vipu-server-timeout 300
      --vipu-server-host=${IPUOF_VIPU_API_HOST}
      --vipu-partition=${IPUOF_VIPU_API_PARTITION_ID}
      --ipus-per-replica 1
      --num-instances 8
      --num-replicas 64
    python3 train.py
      --config efficientnet_b4_g1_with_batch_norm_64ipus
      --identical-replica-seeding
      --epochs 350
      --logs-per-epoch 16
      --epochs-per-ckpt 1
      --no-validation
      --data-dir $DATASETS_DIR
      --wandb
      --seed 1
      --log-dir ./logs/checkpoints
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=./tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"unlimited"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'accuracy: *(.*?) \%'
      skip: 10
    loss:
      reduction_type: 'final'
      regexp: 'loss: *(\d*\.\d*)'
      skip: 10
  output:
    - [samples/sec, 'throughput']
  readme:
    - "applications/tensorflow/cnns/training/README_Benchmarks.md"
    - "Training"
    - "Standard EfficientNet-B4 Training"
    - "1 x IPU-POD64"


tf1_efficientnet_b4_batchnorm_infer_real_pod16:
  description:
    Standard EfficientNet-B4 with batch norm validation, testing convergence on Pod16
  cmd: >-
    python3
      validation.py
      --config efficientnet_b4_g1_with_batch_norm_16ipus
      --data-dir $DATASETS_DIR/imagenet-data
      --restore-path ./logs/checkpoints
      --log-dir ./logs/checkpoints
      --epochs-per-ckpt 1
      --logs-per-epoch 2
      --wandb
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=./tf_cache/"
    POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
    POPLAR_RUNTIME_OPTIONS: '{"streamCallbacks.maxLookahead":"unlimited"}'
  location: training
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 1
    accuracy:
      reduction_type: 'final'
      regexp: 'top-1 accuracy: *(.*?) \%'
  output:
    - [accuracy, 'accuracy']

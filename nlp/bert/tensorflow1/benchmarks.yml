---
# --- Pretraining ---
pretrain_options: &pretrain_options
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/tmp/tf_cache/"
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 3
    mlm_acc:
      reduction_type: "final"
      regexp: 'mlm_acc: *(.*?) \%'
    mlm_loss:
      reduction_type: "final"
      regexp: 'mlm_loss: *(\d*\.\d*)'
    nsp_acc:
      reduction_type: "final"
      regexp: 'nsp_acc: *(.*?) \%'
    nsp_loss:
      reduction_type: "final"
      regexp: 'nsp_loss: *(\d*\.\d*)'
  output:
    - [samples/sec, "throughput"]
    - [mlm_acc, "mlm_acc"]
    - [mlm_loss, "mlm_loss"]
    - [nsp_acc, "nsp_acc"]
    - [nsp_loss, "nsp_loss"]

tf1_bert_large_pretrain_real_pod16:
  <<: *pretrain_options
  description: |
    This tests measures the throughput of Bert Large reference
    config for phase1 pretraining.
  parameters:
    - [config, train_file]
    - ["configs/pretrain_large_128_phase1.json",
       "$DATASETS_DIR/tf_wikipedia/tokenised_128_dup5_mask20/wiki_00_cleaned.tfrecord"]
    - ["configs/pretrain_large_384_phase2.json",
       "$DATASETS_DIR/tf_wikipedia/tokenised_384_dup5_mask58/wiki_00_cleaned.tfrecord"]
  cmd: >-
    python3 run_pretraining.py
        --config {config}
        --train-file {train_file}
        --num-train-steps 4

tf1_bert_base_pretrain_real_pod16:
  <<: *pretrain_options
  description: |
    This tests measures the throughput of Bert Base reference
    config for phase1 pretraining.
  parameters:
    - [config, train_file]
    - ["configs/pretrain_base_128_phase1.json",
        "$DATASETS_DIR/tf_wikipedia/tokenised_128_dup5_mask20/wiki_00_cleaned.tfrecord"]
    - ["configs/pretrain_base_384_phase2.json",
        "$DATASETS_DIR/tf_wikipedia/tokenised_384_dup5_mask58/wiki_00_cleaned.tfrecord"]
  cmd: >-
    python3 run_pretraining.py
      --config {config}
      --train-file {train_file}
      --num-train-steps 4

tf1_bert_large_sl128_pretrain_real_pod64_conv:
  <<: *pretrain_options
  description: |
    Tests convergence of Bert Large reference config for
    phase1 pretraining on 64 IPUs.
  cmd: >-
    poprun
      --vv
      --num-instances 1
      --num-replicas 16
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 4
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
      python3 run_pretraining.py
        --config configs/pretrain_large_128_phase1_POD64.json
        --train-file
        "$DATASETS_DIR/tf_wikipedia/tokenised_128_dup5_mask20/*.tfrecord"
        --save-path "checkpoint/phase1/"
        --steps-per-ckpts 8000
        --steps-per-logs 100
        --wandb
        --wandb-name tf1_bert_large_sl128_pretrain_real_pod64_conv

tf1_bert_large_sl384_pretrain_real_pod64_conv:
  <<: *pretrain_options
  description: |
    Tests performance of Bert Large reference config for
    phase 2 pretraining on 64 IPUs.
  cmd: >-
    poprun
      --vv
      --num-instances 1
      --num-replicas 16
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 4
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 run_pretraining.py
      --config configs/pretrain_large_384_phase2_POD64.json
      --train-file
      $DATASETS_DIR/tf_wikipedia/tokenised_384_dup5_mask58/*.tfrecord
      --init-checkpoint "checkpoint/phase1/ckpt-7031"
      --save-path "checkpoint/phase2/"
      --steps-per-ckpts 8000
      --steps-per-logs 100
      --wandb
      --wandb-name tf1_bert_large_sl384_pretrain_real_pod64_conv

tf1_groupbert_base_pretrain_real_pod16:
  <<: *pretrain_options
  description: |
    This tests measures the throughput of Group BERT-Base reference
    config for phase1 and phase2 pretraining on 16 IPUs.
  parameters:
    phase: 128,384
  cmd: >-
    python3 run_pretraining.py
      --config "configs/groupbert/pretrain_base_groupbert_{phase}.json"
      --train-file $DATASETS_DIR/wikipedia/{phase}/wiki_1[0-1]*.tfrecord
      --num-train-steps 10
      --steps-per-logs 1

tf1_groupbert_base_sl128_pretrain_real_pod64_conv:
  <<: *pretrain_options
  description: |
    Tests convergence of Group BERT-Base reference config for
    phase1 pretraining on 64 IPUs.
  cmd: >-
    poprun
      --mpi-local-args="-x TF_POPLAR_FLAGS"
      --ipus-per-replica 4
      --num-instances 4
      --num-replicas 16
    python3 run_pretraining.py
      --config configs/groupbert/pretrain_base_groupbert_128_POD64.json
      --train-file
        $DATASETS_DIR/tf_wikipedia/tokenised_128_dup5_no_remask/*.tfrecord
      --save-path "checkpoint/phase1/"
      --steps-per-logs 100
      --steps-per-ckpts 8000
      --wandb
      --wandb-name tf1_groupbert_base_sl128_pretrain_real_pod64_conv
      --seed 123

tf1_groupbert_base_sl384_pretrain_real_pod64_conv:
  <<: *pretrain_options
  description: |
    Tests convergence of Group BERT-Base reference config for
    phase 2 pretraining on 64 IPUs.
  cmd: >-
    poprun
      --mpi-global-args="--allow-run-as-root --tag-output"
      --mpi-local-args="-x TF_POPLAR_FLAGS"
      --ipus-per-replica 4
      --num-instances 4
      --num-replicas 16
    python3 run_pretraining.py
      --config configs/groupbert/pretrain_base_groupbert_384_POD64.json
      --train-file
        $DATASETS_DIR/tf_wikipedia/tokenised_384_dup5_no_remask/*.tfrecord
      --init-checkpoint "checkpoint/phase1/ckpt-7110"
      --save-path "checkpoint/phase2/"
      --steps-per-logs 100
      --steps-per-ckpts 2000
      --wandb
      --wandb-name tf1_groupbert_base_sl384_pretrain_real_pod64_conv
      --seed 123

# --- SQuAD training ---
squad_options: &squad_options
  env:
    TF_POPLAR_FLAGS: "--null_data_feed --executable_cache_path=/tmp/tf_cache/"
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 2
  output:
    - [samples/sec, 'throughput']

tf1_bert_squad_large_train_real_pod16:
  <<: *squad_options
  description:
    Tests BERT SQuAD (fine tuning) training throughput
  cmd: >-
    python3 run_squad.py
      --config configs/squad_large.json
      --do-training
      --init-checkpoint ""
      --num-train-steps 4
      --train-file $DATASETS_DIR/squad/train-v1.1.json
      --vocab-file $DATASETS_DIR/ckpts/uncased_L-24_H-1024_A-16/vocab.txt

tf1_groupbert_squad_base_finetune_real_pod4_conv:
  <<: *squad_options
  description:
    Tests BERT SQuAD base (fine tuning) training convergence
  cmd: >-
    poprun
      --vv
      --num-instances 1
      --num-replicas 1
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 4
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 run_squad.py
      --config configs/groupbert/squad/squad_base.json
      --do-training
      --do-predict
      --do-evaluation
      --init-checkpoint "checkpoint/phase2/ckpt-1361"
      --train-file $DATASETS_DIR/squad/train-v1.1.json
      --vocab-file $DATASETS_DIR/ckpts/uncased_L-2_H-128_A-2/vocab.txt
      --predict-file $DATASETS_DIR/squad/dev-v1.1.json
      --wandb
      --wandb-name tf1_groupbert_squad_base_finetune_real_pod4_conv
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec"'
      skip: 2
  output:
    - [samples/sec, 'throughput']

tf1_bert_squad_large_train_real_pod16_conv:
  <<: *squad_options
  description:
    Tests BERT SQuAD (fine tuning) training convergence
  cmd: >-
    poprun
      --vv
      --num-instances 1
      --num-replicas 4
      --update-partition=yes
      --remove-partition=yes
      --reset-partition=no
      --sync-type=ST_POD_NATIVE_DEFAULT
      --vipu-server-timeout 300
      --vipu-server-host $VIPU_CLI_API_HOST
      --vipu-partition=$IPUOF_VIPU_API_PARTITION_ID
      --vipu-cluster=$CLUSTER
      --ipus-per-replica 4
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x IPUOF_VIPU_API_TIMEOUT=600
        -x POPLAR_LOG_LEVEL=WARN
        -x DATASETS_DIR
        -x POPLAR_ENGINE_OPTIONS
        -x POPLAR_TARGET_OPTIONS"
    python3 run_squad.py
      --config configs/squad_large.json
      --do-training
      --do-predict
      --do-evaluation
      --init-checkpoint "checkpoint/phase2/ckpt-2098"
      --train-file $DATASETS_DIR/squad/train-v1.1.json
      --vocab-file $DATASETS_DIR/ckpts/uncased_L-24_H-1024_A-16/vocab.txt
      --predict-file $DATASETS_DIR/squad/dev-v1.1.json
      --wandb
      --wandb-name tf1_bert_squad_large_train_real_pod16_conv
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec"'
      skip: 2
  output:
    - [samples/sec, 'throughput']

# --- GLUE training ---
glue_options: &glue_options
  env:
    TF_POPLAR_FLAGS: "--executable_cache_path=/tmp/tf_cache/"
  data:
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
      skip: 2
    loss:
      regexp: 'loss: *(\d*\.\d*)'
      reduction_type: "final"
  output:
    - [samples/sec, 'throughput']
    - [loss, 'loss']

tf1_glue_base_train_real_pod16:
  <<: *glue_options
  description:
    This benchmark measures the throughput of Bert Base GLUE fine tuning.
  cmd: >-
    python3 run_classifier.py
      --config configs/glue_base.json
      --task-name mrpc
      --do-training
      --init-checkpoint ""
      --num-train-steps 4
      --data-dir $DATASETS_DIR/glue/MRPC

tf1_glue_large_train_real_pod16:
  <<: *glue_options
  description:
    This benchmark measures the throughput of Bert Large GLUE fine tuning.
  cmd: >-
    python3 run_classifier.py
      --config configs/glue_large.json
      --task-name mrpc
      --do-training
      --init-checkpoint ""
      --num-train-steps 4
      --data-dir $DATASETS_DIR/glue/MRPC

# --- Inference ---
tf1_bert_squad_large_infer_gen_2ipu:
  <<: *squad_options
  description:
    Tests BERT SQuAD runtime inference and records the throughput and latency
    on 2 IPUs (1x 2IPU model)
  cmd: >-
    python3 run_squad_runtime.py
      --config configs/squad_large_inference_2ipu.json
      --init-checkpoint ""
      --generated-data
      --vocab-file
      $DATASETS_DIR/tf_wikipedia/ckpts/uncased_L-24_H-1024_A-16/vocab.txt
      --gradient-accumulation-count 80000
      --device-iterations 1000
      --embedded-runtime
      --micro-batch-size 1
      --num-iter 10000
  data:
    latency:
      regexp: 'latency avg: *(.*?) ms'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms'
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
  output:
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']

tf1_bert_squad_large_infer_gen_4ipu:
  <<: *squad_options
  description:
    Tests BERT SQuAD runtime inference and records the throughput and latency
    on 4 IPUs (2x 2IPU models)
  cmd: >-
    poprun
      --vv
      --num-instances 2
      --num-replicas 2
    python3
      run_squad_runtime.py
      --config configs/squad_large_inference_2ipu.json
      --init-checkpoint ""
      --generated-data
      --vocab-file
      $DATASETS_DIR/tf_wikipedia/ckpts/uncased_L-24_H-1024_A-16/vocab.txt
      --gradient-accumulation-count 80000
      --device-iterations 1000
      --embedded-runtime
      --micro-batch-size 1
      --num-iter 10000
  data:
    latency:
      regexp: 'latency avg: *(.*?) ms'
    latency_99p:
      regexp: 'latency 99p: *(.*?) ms'
    latency_99p9:
      regexp: 'latency 99p9: *(.*?) ms'
    throughput:
      regexp: 'throughput: *(.*?) samples\/sec'
  output:
    - [samples/sec, 'throughput']
    - [latency(ms), 'latency']
    - [latency 99th percentile(ms), 'latency_99p']
    - [latency 99.9th percentile(ms), 'latency_99p9']

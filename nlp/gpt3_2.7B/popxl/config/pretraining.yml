# -------- Models --------
"gpt3_2.7B": &gpt3_2-7B
  model:
    hidden_size: 2560
    layers: 32
    sequence_length: 2048
    embedding:
      max_positional_length: 2048
    attention:
      heads: 32
  training:
    global_batch_size: 512
    steps: 286000
tiny: &tiny
  training:
    global_batch_size: 16
    steps: 100000
    optimizer:
      optimizer: adamw
      learning_rate:
        maximum: 0.00001
        warmup_proportion: 0.00625
      weight_decay: 0.01

# -------------------------

# ------- Execution -------
release:
  tiny:
    <<: *tiny
    execution:
      io_tiles: 64
      micro_batch_size: 1
      data_parallel: 2
      tensor_parallel: 4
  "gpt3_2.7B_pod64":
    <<: *gpt3_2-7B
    execution:
      micro_batch_size: 1
      loss_scaling: 2048
      io_tiles: 128
      data_parallel: 8
      tensor_parallel: 8
      available_memory_proportion: [ 0.2 ]
